{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "import pickle\n",
    "import json\n",
    "import vosk\n",
    "import pyaudio\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation():\n",
    "    # Language in which you want to convert\n",
    "    language = 'en'\n",
    "    \n",
    "    # Load the LLaMA 3.2 model from Ollama backend\n",
    "    model = OllamaLLM(model='llama3.2')\n",
    "    \n",
    "    # Load the Vosk model\n",
    "    #modelvoice = vosk.Model(lang=\"en-us\")    \n",
    "    #recognizer = vosk.KaldiRecognizer(model, 16000 )\n",
    "    \n",
    "    # Set up long-term memory using ChromaDB\n",
    "    memory = ConversationBufferMemory()\n",
    "    \n",
    "    # Set up Chroma vector store\n",
    "    #vectorstore = Chroma(persist_directory='./chroma_vectorstore', embedding_function=model.embed_text)\n",
    "    from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "    embeddings = OllamaEmbeddings(model='llama3.2')\n",
    "    vectorstore = Chroma(persist_directory='./chroma_vectorstore', embedding_function=embeddings)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set up retrieval chain for Chroma vectorstore\n",
    "    from langchain.chains import RetrievalQA\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    retriever = vectorstore.as_retriever(search_type='similarity')\n",
    "    retrieval_chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model, language, memory, retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_microphon():\n",
    "    # Start audio stream with error handling for device selection\n",
    "    # Open the microphone stream\n",
    "    device = pyaudio.PyAudio()\n",
    "    stream = device.open(format=pyaudio.paInt16,\n",
    "                         channels=1,\n",
    "                         rate=16000,\n",
    "                         input=True,\n",
    "                         frames_per_buffer=8192\n",
    "                         )\n",
    "        \n",
    "    return device, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tools(retrieval_chain, model, memory):\n",
    "    # Set up search and Wikipedia tools\n",
    "    duckduckgo = DuckDuckGoSearchResults()\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    \n",
    "    # Create a custom tool for the retrieval chain\n",
    "    from langchain.tools import Tool\n",
    "    retrieval_tool = Tool(\n",
    "        name=\"ChromaRetrieval\",\n",
    "        func=lambda q: retrieval_chain.invoke(q),\n",
    "        description=\"Use this tool to retrieve information from Chroma vectorstore\"\n",
    "    )\n",
    "    \n",
    "    # Create the React agent with memory, internet search, and Wikipedia access\n",
    "    agent = initialize_agent(\n",
    "        tools=[retrieval_tool, wikipedia, duckduckgo],\n",
    "        llm=model,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        memory=memory,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        allowed_tools=[\"ChromaRetrieval\", \"wikipedia\", \"duckduckgo\"]\n",
    "    )\n",
    "    \n",
    "    return retrieval_tool, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation with the agent\n",
    "def start_conversation(agent, memory):\n",
    "    \n",
    "    assistant_intro = \"\"\"You are an AI assistant specialized in artificial intelligence, robotics, and automotive technology. \\\n",
    "    Your job is to help generate maximum of three creative ideas and answer questions on these topics. Use the latest research and trends in your responses.\\nExplain exactly three ideas and make your answer short. Stop after the answer.\\n\\nQuestion: \"\"\"\n",
    "\n",
    "    print(\"Agent: How can I help you? Type 'exit' to end the conversation.\")\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            # Before exiting the program\n",
    "            with open('conversation_history.pkl', 'wb') as f:\n",
    "                pickle.dump(memory, f)\n",
    "            break\n",
    "        \n",
    "        # Modify prompt and add user input\n",
    "        modified_prompt = assistant_intro + user_input\n",
    "\n",
    "        # Agent processes user input\n",
    "        response = agent.invoke(modified_prompt)\n",
    "\n",
    "        # Print agent's response\n",
    "        #print(f\"Agent: {response}\")\n",
    "        print(response['output']) if isinstance(response, dict) and 'output' in response else print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: How can I help you? Type 'exit' to end the conversation.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: ChromaRetrieval\n",
      "Action Input: \"automotive agent applications\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_644/1964116756.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  func=lambda q: retrieval_chain.run(q),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm ready to assist. What's the question?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to retrieve information on agents in automotive from the Chroma vectorstore.\n",
      "\n",
      "Action: ChromaRetrieval\n",
      "Action Input: \"automotive agent applications\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'd be happy to help with your question about automotive agent applications! However, I need more context from you to provide a helpful answer. Please provide the specific context or details regarding the automotive industry and what kind of agents are being referred to (e.g. dealer agents, service agent, etc.).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: How can agents used in automotive?\n",
      "\n",
      "Thought: Action: ChromaRetrieval\n",
      "Action Input: \"automotive agent applications\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm ready to help with automotive agent applications. What is the specific application or scenario you'd like me to address?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Agents in automotive can be used for various purposes, such as predictive maintenance, vehicle tracking, and autonomous driving systems. For example, agents can be deployed on vehicles to collect data, perform diagnostics, and provide real-time feedback to technicians. Additionally, agents can be integrated with the vehicle's computer system to enable advanced safety features, such as automatic emergency braking and lane departure warning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agents in automotive can be used for various purposes, such as predictive maintenance, vehicle tracking, and autonomous driving systems. For example, agents can be deployed on vehicles to collect data, perform diagnostics, and provide real-time feedback to technicians. Additionally, agents can be integrated with the vehicle's computer system to enable advanced safety features, such as automatic emergency braking and lane departure warning.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: wikipedia\n",
      "Action Input: \"user interface\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: User interface\n",
      "Summary: In the industrial design field of human–computer interaction, a user interface (UI) is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, while the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls and process controls. The design considerations applicable when creating user interfaces are related to, or involve such disciplines as, ergonomics and psychology.\n",
      "Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i.e. maximum usability). This generally means that the operator needs to provide minimal input to achieve the desired output, and also that the machine minimizes undesired outputs to the user.\n",
      "User interfaces are composed of one or more layers, including a human–machine interface (HMI) that typically interfaces machines with physical input hardware (such as keyboards, mice, or game pads) and output hardware (such as computer monitors, speakers, and printers). A device that implements an HMI is called a human interface device (HID). User interfaces that dispense with the physical movement of body parts as an intermediary step between the brain and the machine use no input or output devices except electrodes alone; they are called brain–computer interfaces (BCIs) or brain–machine interfaces (BMIs).\n",
      "Other terms for human–machine interfaces are man–machine interface (MMI) and, when the machine in question is a computer, human–computer interface. Additional UI layers may interact with one or more human senses, including: tactile UI (touch), visual UI (sight), auditory UI (sound), olfactory UI (smell), equilibria UI (balance), and gustatory UI (taste).\n",
      "Composite user interfaces (CUIs) are UIs that interact with two or more senses. The most common CUI is a graphical user interface (GUI), which is composed of a tactile UI and a visual UI capable of displaying graphics. When sound is added to a GUI, it becomes a multimedia user interface (MUI). There are three broad categories of CUI: standard, virtual and augmented. Standard CUI use standard human interface devices like keyboards, mice, and computer monitors. When the CUI blocks out the real world to create a virtual reality, the CUI is virtual and uses a virtual reality interface. When the CUI does not block out the real world and creates augmented reality, the CUI is augmented and uses an augmented reality interface. When a UI interacts with all human senses, it is called a qualia interface, named after the theory of qualia. CUI may also be classified by how many senses they interact with as either an X-sense virtual reality interface or X-sense augmented reality interface, where X is the number of senses interfaced with. For example, a Smell-O-Vision is a 3-sense (3S) Standard CUI with visual display, sound and smells; when virtual reality interfaces interface with smells and touch it is said to be a 4-sense (4S) virtual reality interface; and when augmented reality interfaces interface with smells and touch it is said to be a 4-sense (4S) augmented reality interface.\n",
      "\n",
      "Page: Graphical user interface\n",
      "Summary: A graphical user interface, or GUI ( GOO-ee), is a form of user interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation. In many applications, GUIs are used instead of text-based UIs, which are based on typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on a compute\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are three creative ideas for enhancing user interfaces?\n",
      "\n",
      "Thought: ChromaRetrieval(\"augmented reality interface\")\n",
      "Action Input: \"interface design\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: Based on the research, here are three creative ideas for enhancing user interfaces:\n",
      "\n",
      "1. **Holographic GUI**: Integrate holographic technology into GUIs to create a more immersive experience. This could allow users to interact with virtual objects and environments in 3D space, making it easier to navigate complex systems.\n",
      "2. **Biometric Feedback Loops**: Incorporate biometric sensors that provide real-time feedback to the user, allowing them to adjust their interface interactions based on their emotional state. For example, if a user is feeling stressed, the system could automatically switch to a calming color scheme or music.\n",
      "3. **Context-Aware Interfaces**: Develop interfaces that adapt to the user's context and environment. For instance, an interface could adjust its layout and brightness based on the time of day or location. This would create a more intuitive and comfortable interaction experience.\n",
      "\n",
      "Please note that these ideas are speculative and require further research and development to become feasible.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the research, here are three creative ideas for enhancing user interfaces:\n",
      "\n",
      "1. **Holographic GUI**: Integrate holographic technology into GUIs to create a more immersive experience. This could allow users to interact with virtual objects and environments in 3D space, making it easier to navigate complex systems.\n",
      "2. **Biometric Feedback Loops**: Incorporate biometric sensors that provide real-time feedback to the user, allowing them to adjust their interface interactions based on their emotional state. For example, if a user is feeling stressed, the system could automatically switch to a calming color scheme or music.\n",
      "3. **Context-Aware Interfaces**: Develop interfaces that adapt to the user's context and environment. For instance, an interface could adjust its layout and brightness based on the time of day or location. This would create a more intuitive and comfortable interaction experience.\n",
      "\n",
      "Please note that these ideas are speculative and require further research and development to become feasible.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # start initialisation\n",
    "    model, language, memory, retrieval_chain = initialisation()\n",
    "    \n",
    "    device, stream = init_microphon()\n",
    "    \n",
    "    # Create the custom tool\n",
    "    retrieval_tool, agent = create_tools(retrieval_chain, model, memory)\n",
    "    \n",
    "    start_conversation(agent, memory)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creativeagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
